(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{173:function(e,t,a){"use strict";a.r(t);var o=a(0),n=Object(o.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"initialize-cluster"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#initialize-cluster","aria-hidden":"true"}},[e._v("#")]),e._v(" Initialize cluster")]),e._v(" "),a("p",[e._v("With the cio software installed on all nodes, the next step is to configure a cluster and then initialize the cluster for use. As part of cluster creation, cio will automatically discover and add drive resources from each node into a storage pool. Drives that are partitioned or have a file system will not be added.")]),e._v(" "),a("p",[e._v("Start configuring a cio storage cluster with the "),a("code",[e._v("cioctl create")]),e._v(" command. Example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c1 ~]# cioctl create\nCluster started. The current node is now the primary controller node. To add a storage node to this cluster, run the following command:\n    cioctl join 192.168.3.95 root f26e695d\n\nAfter adding all storage nodes, return to this node and run following command to initialize the cluster:\n    cioctl init f26e695d\n")])])]),a("p",[e._v("The first node where the "),a("code",[e._v("cioctl create")]),e._v(" command runs becomes the sds controller node (c1 in example above). This node is identified as the sds node in the "),a("code",[e._v("cio node ls")]),e._v(" command.")]),e._v(" "),a("p",[e._v("The output of the create sub-command includes a "),a("code",[e._v("cioctl join")]),e._v(" command to add new nodes to the cluster. Add nodes by running the "),a("code",[e._v("cioctl join")]),e._v(" command on each new node.")]),e._v(" "),a("p",[e._v("Example four node cluster with new nodes c2, c3, c4:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c2 ~]# cioctl join 192.168.3.95 root f26e695d\nAdding this node to cluster as a storage node\n[root@c3 ~]# cioctl join 192.168.3.95 root f26e695d\nAdding this node to cluster as a storage node\n[root@c4 ~]# cioctl join 192.168.3.95 root f26e695d\nAdding this node to cluster as a storage node\n")])])]),a("p",[e._v("Return to the sds controller node and run the "),a("code",[e._v("cioctl init")]),e._v(" command to complete initialization of the cluster.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c1 ~]# cioctl init f26e695d\ncluster: initialization started\n...\ncluster: Node initialization completed\ncluster: Start cio daemon\ncluster: Succeed: Add vd0: Type:3-copy, Size:20GB\ncluster: MongoDB ready\ncluster: Synchronizing VID files\ncluster: Starting API\n")])])]),a("p",[e._v("Note: If you are testing a single-node cluster, skip the "),a("code",[e._v("cioctl join")]),e._v(" command and just run the "),a("code",[e._v("cioctl init")]),e._v(" command.")]),e._v(" "),a("p",[a("strong",[e._v("Initializing bare metal servers with SSDs")])]),e._v(" "),a("p",[e._v("The initialization process will take a few minutes to complete for virtual servers. However when the cio software is installed on physical servers with high performance devices such as SSDs, the first initialization of the cluster will take about 30 minutes. This extra time is used to characterize the available performance. The performance information is used in the quality-of-service (QoS) feature to deliver guaranteed performance for individual applications.")]),e._v(" "),a("p",[a("strong",[e._v("Ready To Use")])]),e._v(" "),a("p",[e._v("If Kubernetes is not detected, the cio software will automatically configure a Docker Swarm cluster. Example below shows a Swarm cluster with three manager nodes and one worker node.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c1 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\ngpx9996b1usy7a0h6cd686g62 *   c1                  Ready               Active              Leader\np917q3v1w3gapqx2zn87652f3     c2                  Ready               Active              Reachable\nvelj1g30557mhayy1hkoqqc75     c3                  Ready               Active              Reachable\njw4robjsehwzw7en48rw2mjie     c4                  Ready               Active\n")])])]),a("p",[e._v("At the end of initialization, a Portainer service is launched to provide an GUI for cluster management. Example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c1 ~]# docker service ps portainer\nID                  NAME                IMAGE                        NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\n9jpoaen6ddke        portainer.1         portainer/portainer:latest   c1                  Running             Running 8 minutes ago\n")])])]),a("p",[e._v("Login to the Portainer UI by pointing your browser at any node IP and port 9000. You can confirm the node IPs with the "),a("code",[e._v("cio node ls")]),e._v(" command:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("[root@c1 ~]# cio node ls\nNODENAME             IP                NODE_ID    ROLE       STATUS\nc1                   192.168.3.95      4132353b   sds        normal\nc2                   192.168.3.53      dceacd20   backup1    normal\nc3                   192.168.3.145     9ee22782   backup2    normal\nc4                   192.168.3.129     d2004822   standard   normal\n")])])]),a("p",[e._v("In this example, point the browser at 192.168.3.95:9000, where 9000 is the default Portainer service port number.")])])}],!1,null,null,null);t.default=n.exports}}]);